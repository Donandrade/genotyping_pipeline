## 1. Project Overview

This pipeline performs **read trimming, alignment, QC, per-sample variant calling, per-chromosome merging**, and **optional probe-based filtering**.  
It is designed for execution as a SLURM array job but can be adapted for other environments.

---

## 2. Getting started

Clone this repository into your SLURM-based HPC account before running the pipeline.

```bash
git clone https://github.com/Donandrade/genotyping_pipeline.git
```

## 3. Workflow summary

The workflow includes:

- Trimming of raw FASTQ files

- Alignment using BWA-MEM

- Read-group assignment

- BAM post-processing and QC

- mpileup generation (optionally restricted to probe regions)

- Chromosome-wise merging of pileups

- Optional integration of probe BED regions

- Output reporting (flagstat + summary TSV)


## 4. Required Input Files and directories

- `samples.tsv`

```bash
sample_id   r1                          r2
sample001   fastq/sample001_R1.fq.gz    fastq/sample001_R2.fq.gz
sample002   fastq/sample002_R1.fq.gz    fastq/sample002_R2.fq.gz
```

- `probes.bed (optional)`

```bash
VaccDscaff1	0	300	VaccDscaff1_probe001
VaccDscaff1	1000	1300	VaccDscaff1_probe002
VaccDscaff1	2000	2300	VaccDscaff1_probe003
VaccDscaff1	3000	3300	VaccDscaff1_probe004
```

- `chrom_size.tsv`

```bash
VaccDscaff1   42640288
VaccDscaff2   37844821
```

- Reference fasta + index files

- FASTQ directory


## 5. Configuration in `genotyping.sh`

- `SAMPLES_TSV="samples.tsv"`
- `PROBES="" #Provide a probes file to restrict bcftools mpileup and bcftools merge to probe regions only`
- `CHROM_SIZE="chrom_size.txt"`
- `REFERENCE="reference/subgenome_blue.multi.fa"`

### Configuring the Array and `PER_TASK`

The number of array tasks (`#SBATCH --array`) and the value of `PER_TASK` must be adjusted according to the size of your dataset.

- `#SBATCH --array` defines how many tasks will run in parallel.
- `PER_TASK` defines how many samples each task will process.

Ensure that: (number of array tasks) × PER_TASK >= total number of samples.  Adjust these values according to your dataset size and available computational resources.



## 6. Running

```bash
sbatch genotyping.sh
```

## 7. Outputs Generated by the Pipeline

The pipeline produces the following groups of files:

### **1. Trimmed FASTQ files**
- `out/<sample>_R1_paired.fq.gz`
- `out/<sample>_R2_paired.fq.gz`

### **2. Aligned BAM files + index**
- `out/bam/<sample>.sorted.group.bam`
- `out/bam/<sample>.sorted.group.bam.bai`

### **3. QC reports (per sample)**
Located in `out/bam_tmp/`:
- `<sample>.flagstat.txt`
- `<sample>.stats.txt`
- `<sample>.idxstats.txt`
- `<sample>.bamvalidate.txt`

### **4. Per-sample VCFs**
- `out/pileup/<sample>_sorted_norm_split.vcf.gz`

### **5. Per-chromosome split pileups**
- `out/pileup/split_chr/<sample>.<CHR>.vcf.gz`

### **6. Merged VCFs (per chromosome/region)**
- `out/merge/merged.<CHR>.all.vcf.gz`  
  or  
- `out/merge/merged.<CHR>.probes.vcf.gz` *(if probes are used)*

### **7. Genotype-called VCF after merge**
- `out/merge/merged.<CHR>.called.vcf.gz`

### **8. Summary reports**
Located in `reports/`:
- `read_counts.tsv` — raw vs trimmed read counts  
- `flagstat_summary.tsv` — QC summary per sample  
- `timing_samples.tsv` and `timing_merge.tsv` — runtime tracking  
- `table_snps_count_last_by_scaffold.tsv` — SNP summary per chromosome (optional)

---

## Directory Structure Created Automatically

```bash
out/
├── bam/
│ ├── <sample>.sorted.group.bam
│ └── <sample>.sorted.group.bam.bai
│
├── bam_tmp/
│ ├── <sample>.flagstat.txt
│ ├── <sample>.stats.txt
│ ├── <sample>.idxstats.txt
│ └── <sample>.bamvalidate.txt
│
├── pileup/
│ ├── <sample>_sorted_norm_split.vcf.gz
│ ├── <sample>_sorted_norm_split.vcf.gz.tbi
│ │
│ └── split_chr/
│ ├── <sample>.<CHR>.vcf.gz
│ └── <sample>.<CHR>.vcf.gz.tbi
│
└── merge/
├── merged.<CHR>.all.vcf.gz
├── merged.<CHR>.probes.vcf.gz
├── merged.<CHR>.called.vcf.gz
└── *.tbi

```
