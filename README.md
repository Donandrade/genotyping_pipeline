## 1. Project Overview

This pipeline performs **read trimming, alignment, QC, per-sample variant calling, per-chromosome merging**, and **optional probe-based filtering**.  
It is designed for execution as a SLURM array job but can be adapted for other environments.

---

## 2. Getting started

Clone this repository into your SLURM-based HPC account before running the pipeline.

```bash
git clone https://github.com/Donandrade/genotyping_pipeline.git
```

## 3. Workflow summary

The workflow includes:

- Trimming of raw FASTQ files

- Alignment using BWA-MEM

- Read-group assignment

- BAM post-processing and QC

- mpileup generation (optionally restricted to probe regions)

- Chromosome-wise merging of pileups

- Optional integration of probe BED regions

- Output reporting (flagstat + summary TSV)


## 4. Required Input Files and directories.

See all general description of each file and it configuration in the next topc (5. Configuration in `genotyping.sh`).

- `samples.tsv`

```bash
sample_id   r1                          r2
sample001   fastq/sample001_R1.fq.gz    fastq/sample001_R2.fq.gz
sample002   fastq/sample002_R1.fq.gz    fastq/sample002_R2.fq.gz
```

- `probes.bed (optional)`

```bash
VaccDscaff1	0	300	VaccDscaff1_probe001
VaccDscaff1	1000	1300	VaccDscaff1_probe002
VaccDscaff1	2000	2300	VaccDscaff1_probe003
VaccDscaff1	3000	3300	VaccDscaff1_probe004
```

- `chrom_size.tsv`

```bash
VaccDscaff1   42640288
VaccDscaff2   37844821
```

- Reference fasta + index files

```bash
reference/subgenome_blue.multi.fa
```

- FASTQ directory

```bash
fastq/
```

## 5. Configuration in `genotyping.sh`

Open the file `genotyping.sh` (The bash workflow used to run the full analysis) and set the folowing variables according your datasate:

- `SAMPLES_TSV="samples.tsv"`  
  Path to the sample table. This TSV file must contain the sample ID and the paths to the R1 and R2 FASTQ files for each sample.

- `PROBES=""  # Provide a probes file to restrict bcftools mpileup and bcftools merge to probe regions only`  
  Optional BED file with probe regions. If set, `bcftools mpileup` and `bcftools merge` will be restricted to these regions.  
  Leave it empty (`""`) to use the entire genome.

- `CHROM_SIZE="chrom_size.txt"`  
  File with chromosome/region names and sizes, separated by a tab. Used to guide per-chromosome processing and variant counting.

- `REFERENCE="reference/subgenome_blue.multi.fa"`  
  Path to the reference genome FASTA file used for alignment and mpileup. Make sure the required index files (e.g. BWA and `.fai`) are available.

- `CHR_LIST=("VaccDscaff1:42640288-42650287"
    "VaccDscaff2:28801683-28811682"
    "VaccDscaff4:21204352-21214351"
    "VaccDscaff6:11534481-11544480"
    "VaccDscaff7:3282650-3292649"
    "VaccDscaff11:27652190-27662189"
    "VaccDscaff12:6795405-6805404"
    "VaccDscaff13:8612145-8622144"
    "VaccDscaff17:2834352-2844351"
    "VaccDscaff20:13144615-13154614"
    "VaccDscaff21:7405786-7415785"
    "VaccDscaff22:28175518-28185517"
  )`
Bash array with the chromosomes or regions to be processed. Each entry must match a valid header/region in the reference FASTA.
The number of elements in CHR_LIST should be consistent with the SLURM array size (#SBATCH --array).

- `ADAPTER_PE="${HPC_TRIMMOMATIC_ADAPTER}/TruSeq3-PE.fa"`
Path to the Trimmomatic adapter file for paired-end reads. Adjust according to your adapter set and environment.

- `TRIM_OPTS_COMMON="SLIDINGWINDOW:4:20 TRAILING:20 MINLEN:50"`
Common Trimmomatic trimming options applied to all samples. You can change these values to match your trimming strategy.

- `USE_PREV_PILEUPS=true`
If true, previously generated pileups listed in PILEUP_TSV will be included in the merge step.
Set to false to ignore previous pileups and use only the newly generated ones.

- `PILEUP_TSV="old_pileup.list"`
List/TSV file with paths to previously generated pileup files to be reused when `USE_PREV_PILEUPS=true`. Keep this file split by chromosome..


### Configuring the Array and `PER_TASK`

The number of array tasks (`#SBATCH --array`) and the value of `PER_TASK` must be adjusted according to the size of your dataset.

- `#SBATCH --array` defines how many tasks will run in parallel.
- `PER_TASK` defines how many samples each task will process.

Ensure that: (number of array tasks) × PER_TASK >= total number of samples.  Adjust these values according to your dataset size and available computational resources.



## 6. Running

```bash
sbatch genotyping.sh
```

## 7. Outputs Generated by the Pipeline

The pipeline produces the following groups of files:

### **1. Trimmed FASTQ files**
- `out/<sample>_R1_paired.fq.gz`
- `out/<sample>_R2_paired.fq.gz`

### **2. Aligned BAM files + index**
- `out/bam/<sample>.sorted.group.bam`
- `out/bam/<sample>.sorted.group.bam.bai`

### **3. QC reports (per sample)**
Located in `out/bam_tmp/`:
- `<sample>.flagstat.txt`
- `<sample>.stats.txt`
- `<sample>.idxstats.txt`
- `<sample>.bamvalidate.txt`

### **4. Per-sample VCFs**
- `out/pileup/<sample>_sorted_norm_split.vcf.gz`

### **5. Per-chromosome split pileups**
- `out/pileup/split_chr/<sample>.<CHR>.vcf.gz`

### **6. Merged VCFs (per chromosome/region)**
- `out/merge/merged.<CHR>.all.vcf.gz`  
  or  
- `out/merge/merged.<CHR>.probes.vcf.gz` *(if probes are used)*

### **7. Genotype-called VCF after merge**
- `out/merge/merged.<CHR>.called.vcf.gz`

### **8. Summary reports**
Located in `reports/`:
- `read_counts.tsv` — raw vs trimmed read counts  
- `flagstat_summary.tsv` — QC summary per sample  
- `timing_samples.tsv` and `timing_merge.tsv` — runtime tracking  
- `table_snps_count_last_by_scaffold.tsv` — SNP summary per chromosome (optional)

---

## Directory Structure Created Automatically

```tree
out/
├── trimmomatic/
│   ├── <sample>_R1_paired.fq.gz
│   ├── <sample>_R1_unpaired.fq.gz
│   ├── <sample>_R2_paired.fq.gz
│   └── <sample>_R2_unpaired.fq.gz
│
├── bam/
│   ├── <sample>.sorted.group.bam
│   └── <sample>.sorted.group.bam.bai
│
├── bam_tmp/
│   ├── <sample>.flagstat.txt
│   ├── <sample>.stats.txt
│   ├── <sample>.idxstats.txt
│   └── <sample>.bamvalidate.txt
│
├── pileup/
│   ├── <sample>_sorted_norm_split.vcf.gz
│   ├── <sample>_sorted_norm_split.vcf.gz.tbi
│   └── split_chr/
│       ├── <sample>.<CHR>.vcf.gz
│       └── <sample>.<CHR>.vcf.gz.tbi
│
└── merge/
    ├── merged.<CHR>.all.vcf.gz
    ├── merged.<CHR>.probes.vcf.gz
    ├── merged.<CHR>.called.vcf.gz
    └── *.tbi
```
